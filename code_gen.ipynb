{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPNV2p6rMWoOCSodl7mJwLP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a7690623ec044d228b0bb3bb7e37f3ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f017d9ec51304d03b421f28ea5df64ec",
              "IPY_MODEL_381fdf5782f34c16bd8c85697bdeb25c",
              "IPY_MODEL_68d20171b12a4f1a911d5383a5270daa"
            ],
            "layout": "IPY_MODEL_607ae5827be54712b802ae063a3af3f8"
          }
        },
        "f017d9ec51304d03b421f28ea5df64ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64f8d0d8c01746e190e5c59e3bc4948d",
            "placeholder": "​",
            "style": "IPY_MODEL_3f7fe4e13f614ff2b0cd948701be0c50",
            "value": "Map: 100%"
          }
        },
        "381fdf5782f34c16bd8c85697bdeb25c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2f41f1b4c5e43efa1b96c202d37fa5d",
            "max": 876,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_431ecff4cc124655a407c4c8ada5e6ca",
            "value": 876
          }
        },
        "68d20171b12a4f1a911d5383a5270daa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fd76edd3ae5479d8b2f6212cbf5e85d",
            "placeholder": "​",
            "style": "IPY_MODEL_ac22081c7ba5410cb9ad9d1a3d679550",
            "value": " 876/876 [00:01&lt;00:00, 670.81 examples/s]"
          }
        },
        "607ae5827be54712b802ae063a3af3f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64f8d0d8c01746e190e5c59e3bc4948d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f7fe4e13f614ff2b0cd948701be0c50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2f41f1b4c5e43efa1b96c202d37fa5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "431ecff4cc124655a407c4c8ada5e6ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fd76edd3ae5479d8b2f6212cbf5e85d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac22081c7ba5410cb9ad9d1a3d679550": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02bc4a30a7a5448ea72fec1b608cb446": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95055733eefb495e9d4005cd8948735b",
              "IPY_MODEL_6315821ac76d460f8d6a88c32e155905",
              "IPY_MODEL_2996b93b86cd458f8ece20b311e4dc67"
            ],
            "layout": "IPY_MODEL_4dfa953210984cffb96396fcb6b878f6"
          }
        },
        "95055733eefb495e9d4005cd8948735b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4a19bc7a3cc452c83ec4b9d9c430467",
            "placeholder": "​",
            "style": "IPY_MODEL_ab2eaba806af4d1b9bf7b31deb7663ef",
            "value": "Map: 100%"
          }
        },
        "6315821ac76d460f8d6a88c32e155905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc53fe16d7bd4636a39afa3de0b8d283",
            "max": 98,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60eeeacf93864fcd949289bb8c258014",
            "value": 98
          }
        },
        "2996b93b86cd458f8ece20b311e4dc67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4c7e4b7b26b4a8a90efa98e3f05ca73",
            "placeholder": "​",
            "style": "IPY_MODEL_6365e973baec447f9f03d1cec933714f",
            "value": " 98/98 [00:00&lt;00:00, 535.46 examples/s]"
          }
        },
        "4dfa953210984cffb96396fcb6b878f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4a19bc7a3cc452c83ec4b9d9c430467": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab2eaba806af4d1b9bf7b31deb7663ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc53fe16d7bd4636a39afa3de0b8d283": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60eeeacf93864fcd949289bb8c258014": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4c7e4b7b26b4a8a90efa98e3f05ca73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6365e973baec447f9f03d1cec933714f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nidhalamara/CodeGenerator/blob/main/code_gen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR5hy5zi0aDJ",
        "outputId": "ead74266-92ab-42cb-eeff-52f65086bd74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_0fVv0Az2iY",
        "outputId": "9d7fcd0b-3a44-4d70-ae0d-932d97338607"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "WARNING:jax._src.xla_bridge:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_flax_utils.py:24: FutureWarning: Importing `FlaxGenerationMixin` from `src/transformers/generation_flax_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import FlaxGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version 2.14.0\n",
            "Num GPUs Available:  0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"  # reduce the amount of console output from TF\n",
        "import tensorflow as tf\n",
        "\n",
        "from transformers import *\n",
        "!pip install -q datasets # install HF datasets library\n",
        "from datasets import load_dataset\n",
        "\n",
        "logging.set_verbosity_warning()\n",
        "logging.set_verbosity_error()\n",
        "\n",
        "import logging\n",
        "\n",
        "print('TF version',tf.__version__)\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU'))) # check GPU available"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_strategy(xla, fp16, no_cuda):\n",
        "    print(\" Tensorflow: setting up strategy\")\n",
        "\n",
        "    # setup xla\n",
        "    if xla:\n",
        "        print(\" XLA Enabled\")\n",
        "        tf.config.optimizer.set_jit(True)\n",
        "\n",
        "    # setup mixed precision training\n",
        "    if fp16:\n",
        "        # Set to float16 at first\n",
        "        print(\" Mixed Precision Training Enabled\")\n",
        "        policy = tf.keras.mixed_precision.experimental.Policy(\"mixed_float16\")\n",
        "        tf.keras.mixed_precision.experimental.set_policy(policy)\n",
        "\n",
        "    # setup distribution strategy\n",
        "    gpus = tf.config.list_physical_devices(\"GPU\")\n",
        "    if no_cuda:\n",
        "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "    else:\n",
        "      if len(gpus) == 0:\n",
        "            print(\" One Device Strategy [CPU] Enabled\")\n",
        "            strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
        "      elif len(gpus) == 1:\n",
        "            print(\" One Device Strategy [GPU] Enabled\")\n",
        "            strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
        "      elif len(gpus) > 1:\n",
        "            print(\" Mirrored Strategy Enabled\")\n",
        "            # If only want to use a specific subset of GPUs use CUDA_VISIBLE_DEVICES=0`\n",
        "            strategy = tf.distribute.MirroredStrategy()\n",
        "      else:\n",
        "            strategy = tf.distribute.get_strategy()\n",
        "\n",
        "    return strategy\n",
        "\n",
        "def n_replicas(strategy):\n",
        "    # return number of devices\n",
        "    return strategy.num_replicas_in_sync\n",
        "\n",
        "# note:\n",
        "# huggingface TF-T5 implementation has issues when mixed precision is enabled\n",
        "# we will disable FP16 for this but can be used for training any other model\n",
        "strategy = setup_strategy(xla=True, fp16=False, no_cuda=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmqV1IR70wdX",
        "outputId": "81cdf6e9-2f73-4fb9-81b8-833aaa146315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Tensorflow: setting up strategy\n",
            " XLA Enabled\n",
            " One Device Strategy [CPU] Enabled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_dataset(cache_dir):\n",
        "    # download data using a keras utility\n",
        "    _url = \"https://raw.githubusercontent.com/google-research/google-research/master/mbpp/mbpp.jsonl\" # download mbpp dataset\n",
        "    dataset_path = tf.keras.utils.get_file(\"mbpp.jsonl\", origin=_url, cache_dir=cache_dir, cache_subdir=cache_dir)\n",
        "    return dataset_path\n",
        "\n",
        "def convert_examples_to_features(examples, tokenizer, args):\n",
        "    # encode text-code pairs\n",
        "    texts = examples['text']\n",
        "    codes = examples['code']\n",
        "    # tests = [\" \".join(test) for test in examples['test_list']] # convert list of test cases to single string\n",
        "\n",
        "    # encode texts by prepending the task for input sequence\n",
        "    inputs = [args.prefix + text for text in texts]\n",
        "    model_inputs = tokenizer(inputs, max_length=args.max_input_length, padding=\"max_length\", truncation=True)\n",
        "    # encode texts by prepending the task for input sequence and appending the test sequence\n",
        "    # inputs = [args.prefix + text + \" \" + test for text, test in zip(texts, tests)]\n",
        "    # model_inputs = tokenizer(inputs, max_length=args.max_input_length, padding=\"max_length\", truncation=True)\n",
        "\n",
        "    # encode texts by prepending the task for input sequence\n",
        "    labels = tokenizer(codes, max_length=args.max_target_length, padding=\"max_length\", truncation=True).input_ids\n",
        "\n",
        "    # we need to replace the index of the padding tokens by -100\n",
        "    # such that they are not taken into account by the CrossEntropyLoss\n",
        "    labels_with_ignore_index = []\n",
        "    for labels_example in labels:\n",
        "        labels_example = [label if label != 0 else -100 for label in labels_example]\n",
        "        labels_with_ignore_index.append(labels_example)\n",
        "    model_inputs[\"labels\"] = labels_with_ignore_index\n",
        "\n",
        "    # return features\n",
        "    return model_inputs\n"
      ],
      "metadata": {
        "id": "rlf0f6sf0wg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_tfdataset(train_dataset, num_train_examples, args):\n",
        "    # select feature columns\n",
        "    columns = ['input_ids', 'attention_mask', 'labels']\n",
        "    # set to tensorflow format\n",
        "    train_dataset.set_format(type='tensorflow', columns=columns)\n",
        "\n",
        "    # specify return types\n",
        "    return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, 'labels':tf.int32}\n",
        "    # specify return shapes\n",
        "    return_shapes = {'input_ids': tf.TensorShape([None]),'attention_mask': tf.TensorShape([None]), 'labels': tf.TensorShape([None])}\n",
        "    # initialize dataset\n",
        "    tf_dataset = tf.data.Dataset.from_generator(lambda : train_dataset, return_types, return_shapes)\n",
        "\n",
        "    # turn off auto-sharding\n",
        "    options = tf.data.Options()\n",
        "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
        "    tf_dataset = tf_dataset.with_options(options)\n",
        "    ds = (\n",
        "        tf_dataset.repeat()\n",
        "        .shuffle(num_train_examples, seed=args.seed)\n",
        "        .batch(args.train_batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "    return strategy.experimental_distribute_dataset(ds)"
      ],
      "metadata": {
        "id": "aqvY6Gho0wkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_validation_tfdataset(eval_dataset, num_validation_examples, args):\n",
        "    # select feature columns\n",
        "    columns = ['input_ids', 'attention_mask', 'labels']\n",
        "    # set to tensorflow format\n",
        "    eval_dataset.set_format(type='tensorflow', columns=columns)\n",
        "\n",
        "    # specify return types\n",
        "    return_types = {'input_ids':tf.int32, 'attention_mask':tf.int32, 'labels':tf.int32}\n",
        "    # specify return shapes\n",
        "    return_shapes = {'input_ids': tf.TensorShape([None]),'attention_mask': tf.TensorShape([None]), 'labels': tf.TensorShape([None])}\n",
        "    # initialize dataset\n",
        "    tf_dataset = tf.data.Dataset.from_generator(lambda : eval_dataset, return_types, return_shapes)\n",
        "\n",
        "    # turn off auto-sharding\n",
        "    options = tf.data.Options()\n",
        "    options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
        "    tf_dataset = tf_dataset.with_options(options)\n",
        "    ds = (\n",
        "        tf_dataset.repeat()\n",
        "        .batch(args.validation_batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "\n",
        "    # distribute dataset to devices\n",
        "    return strategy.experimental_distribute_dataset(ds)\n"
      ],
      "metadata": {
        "id": "oW88hWbP0wnd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_all_seeds(seed):\n",
        "    # set random seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "\n",
        "def init_logger(log_file=None, log_file_level=logging.NOTSET):\n",
        "    # initialize logger for tracking events and save in file\n",
        "    if isinstance(log_file, Path):\n",
        "        log_file = str(log_file)\n",
        "    log_format = logging.Formatter(\n",
        "        fmt='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
        "        datefmt='%m/%d/%Y %H:%M:%S'\n",
        "    )\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.INFO)\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setFormatter(log_format)\n",
        "    logger.handlers = [console_handler]\n",
        "    if log_file and log_file != '':\n",
        "        file_handler = logging.FileHandler(log_file)\n",
        "        file_handler.setLevel(log_file_level)\n",
        "        # file_handler.setFormatter(log_format)\n",
        "        logger.addHandler(file_handler)\n",
        "    return logger"
      ],
      "metadata": {
        "id": "IcjloJQ_1cEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProgressBar(object):\n",
        "    # custom progress bar\n",
        "    def __init__(self, n_total,width=30,desc = 'Training'):\n",
        "        self.width = width\n",
        "        self.n_total = n_total\n",
        "        self.start_time = time.time()\n",
        "        self.desc = desc\n",
        "\n",
        "    def __call__(self, step, info={}):\n",
        "        now = time.time()\n",
        "        current = step + 1\n",
        "        recv_per = current / self.n_total\n",
        "        bar = f'[{self.desc}] {current}/{self.n_total} ['\n",
        "        if recv_per >= 1:\n",
        "            recv_per = 1\n",
        "        prog_width = int(self.width * recv_per)\n",
        "        if prog_width > 0:\n",
        "            bar += '=' * (prog_width - 1)\n",
        "            if current< self.n_total:\n",
        "                bar += \">\"\n",
        "            else:\n",
        "                bar += '='\n",
        "        bar += '.' * (self.width - prog_width)\n",
        "        bar += ']'\n",
        "        show_bar = f\"\\r{bar}\"\n",
        "        time_per_unit = (now - self.start_time) / current\n",
        "        if current < self.n_total:\n",
        "            eta = time_per_unit * (self.n_total - current)\n",
        "            if eta > 3600:\n",
        "                eta_format = ('%d:%02d:%02d' %\n",
        "                              (eta // 3600, (eta % 3600) // 60, eta % 60))\n",
        "            elif eta > 60:\n",
        "                eta_format = '%d:%02d' % (eta // 60, eta % 60)\n",
        "            else:\n",
        "                eta_format = '%ds' % eta\n",
        "            time_info = f' - ETA: {eta_format}'\n",
        "        else:\n",
        "            if time_per_unit >= 1:\n",
        "                time_info = f' {time_per_unit:.1f}s/step'\n",
        "            elif time_per_unit >= 1e-3:\n",
        "                time_info = f' {time_per_unit * 1e3:.1f}ms/step'\n",
        "            else:\n",
        "                time_info = f' {time_per_unit * 1e6:.1f}us/step'\n",
        "\n",
        "        show_bar += time_info\n",
        "        if len(info) != 0:\n",
        "          show_info = f'{show_bar} ' + \\\n",
        "                        \"-\".join([f' {key}: {value:.4f} ' if key != \"learning_rate\" else f' {key}: {value:.8f} ' for key, value in info.items()])\n",
        "          print(show_info, end='')\n",
        "        else:\n",
        "            print(show_bar, end='')"
      ],
      "metadata": {
        "id": "Q76qyCiD1cHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    def __init__(\n",
        "        self, model, args, train_dataset, validation_dataset,\n",
        "        num_train_examples, num_validation_examples\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.args = args\n",
        "\n",
        "        self.train_dataset = train_dataset\n",
        "        self.num_train_examples = num_train_examples\n",
        "\n",
        "        self.validation_dataset = validation_dataset\n",
        "        self.num_validation_examples = num_validation_examples\n",
        "\n",
        "        self.global_step = 0\n",
        "        self.eval_loss = tf.keras.metrics.Sum()\n",
        "\n",
        "    def create_optimizer_and_scheduler(self, num_training_steps):\n",
        "      # creates an optimizer with a learning rate schedule using a warmup phase followed by a linear decay.\n",
        "        num_warmup_steps = math.ceil(num_training_steps * self.args.warmup_ratio)\n",
        "        self.optimizer, self.lr_scheduler = create_optimizer(\n",
        "            init_lr=self.args.learning_rate,\n",
        "            num_train_steps=num_training_steps,\n",
        "            num_warmup_steps=num_warmup_steps,\n",
        "            weight_decay_rate=self.args.weight_decay,\n",
        "            adam_epsilon=self.args.adam_epsilon\n",
        "        )\n",
        "\n",
        "    def evaluation_step(self, features, labels, nb_instances_in_global_batch):\n",
        "        # forward pass\n",
        "        outputs = self.model(input_ids=features['input_ids'], attention_mask=features['attention_mask'], labels=labels, training=False)[:2]\n",
        "        loss, logits = outputs[:2]\n",
        "        # loss scaling\n",
        "        scaled_loss = loss / tf.cast(nb_instances_in_global_batch, dtype=loss.dtype)\n",
        "        # add current batch loss\n",
        "        self.eval_loss.update_state(scaled_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def distributed_evaluation_steps(self, batch):\n",
        "        features = {k: v for k, v in batch.items() if 'labels' not in k}\n",
        "        labels = batch['labels']\n",
        "        nb_instances = tf.reduce_sum(tf.cast(labels != -100, dtype=tf.int32))\n",
        "        # strategy.run() expects args to be a list or tuple\n",
        "        inputs = (features, labels, nb_instances)\n",
        "        # `run` replicates the provided computation and runs with the distributed input\n",
        "        strategy.run(self.evaluation_step, inputs)\n",
        "\n",
        "    def evaluate(self):\n",
        "        # calculate total validation steps\n",
        "        steps = math.ceil(self.num_validation_examples / self.args.validation_batch_size)\n",
        "        # reset eval loss after every epoch\n",
        "        self.eval_loss.reset_states()\n",
        "        logs = {}\n",
        "        pbar = ProgressBar(n_total=steps, desc='Evaluating')\n",
        "        # iterate over validation dataset\n",
        "        for step, batch in enumerate(self.validation_dataset):\n",
        "            # distributed evaluation step\n",
        "            self.distributed_evaluation_steps(batch)\n",
        "            logs[\"eval_loss\"] = self.eval_loss.result() / (step + 1)\n",
        "            pbar(step=step, info=logs)\n",
        "            if step == steps - 1:\n",
        "                break\n",
        "        print(\"\\n------------- validation result -----------------\")\n",
        "\n",
        "    def apply_gradients(self, features, labels, nb_instances_in_global_batch):\n",
        "        # forward pass\n",
        "        outputs = self.model(input_ids=features['input_ids'], attention_mask=features['attention_mask'], labels=labels, training=True)[:2]\n",
        "        loss, logits = outputs[:2]\n",
        "        # loss scaling\n",
        "        scaled_loss = loss / tf.cast(nb_instances_in_global_batch, dtype=loss.dtype)\n",
        "        # calculate gradients\n",
        "        gradients = tf.gradients(scaled_loss, self.model.trainable_variables)\n",
        "        # convert gradients with nan value\n",
        "        gradients = [g if g is not None else tf.zeros_like(v) for g, v in zip(gradients, self.model.trainable_variables)]\n",
        "        # optimize the model\n",
        "        self.optimizer.apply_gradients(list(zip(gradients, self.model.trainable_variables)))\n",
        "        # add current batch loss\n",
        "        self.train_loss.update_state(scaled_loss)\n",
        "\n",
        "    @tf.function\n",
        "    def distributed_training_steps(self, batch):\n",
        "        with strategy.scope():\n",
        "            features = {k: v for k, v in batch.items() if 'labels' not in k}\n",
        "            labels = batch['labels']\n",
        "            nb_instances = tf.reduce_sum(tf.cast(labels != -100, dtype=tf.int32))\n",
        "            # strategy.run() expects args to be a list or tuple\n",
        "            inputs = (features, labels, nb_instances)\n",
        "            # `run` replicates the provided computation and runs with the distributed input.\n",
        "            strategy.run(self.apply_gradients, inputs)\n",
        "\n",
        "    def train(self):\n",
        "        # calculate total training steps\n",
        "        num_updates_per_epoch = self.num_train_examples // args.train_batch_size\n",
        "        self.steps_per_epoch = num_updates_per_epoch\n",
        "        t_total = self.steps_per_epoch * self.args.epochs\n",
        "\n",
        "        with strategy.scope():\n",
        "          # optimizer, and checkpoint must be created under `strategy.scope`\n",
        "            # create optimizer and scheduler\n",
        "            self.create_optimizer_and_scheduler(num_training_steps=t_total)\n",
        "\n",
        "            # create checkpoint manager\n",
        "            folder = os.path.join(self.args.output_dir, self.args.checkpoint_dir)\n",
        "            ckpt = tf.train.Checkpoint(optimizer=self.optimizer, model=self.model)\n",
        "            self.model.ckpt_manager = tf.train.CheckpointManager(ckpt, folder, max_to_keep=1)\n",
        "            iterations = self.optimizer.iterations\n",
        "\n",
        "            logger.info(\"***** Running training *****\")\n",
        "            logger.info(f\"  Num examples = {self.num_train_examples}\")\n",
        "            logger.info(f\"  Num Epochs = {self.args.epochs}\")\n",
        "            logger.info(f\"  Total train batch size (w. parallel & distributed) = {self.args.train_batch_size * n_replicas(strategy)}\")\n",
        "            logger.info(f\"  Steps per epoch = {self.steps_per_epoch}\")\n",
        "            logger.info(f\"  Total optimization steps = {t_total}\")\n",
        "\n",
        "            self.train_loss = tf.keras.metrics.Sum(name=\"training_loss\")\n",
        "            start_time = datetime.datetime.now()\n",
        "            for epoch_iter in range(self.args.epochs):\n",
        "                # training loop\n",
        "                logger.info(f\"Epoch {epoch_iter + 1}/{self.args.epochs}\")\n",
        "\n",
        "                pbar = ProgressBar(n_total=self.steps_per_epoch, desc='Training')\n",
        "                # iterate over training dataset\n",
        "                for step, batch in enumerate(self.train_dataset):\n",
        "                    # distributed training step\n",
        "                    self.distributed_training_steps(batch)\n",
        "\n",
        "                    self.global_step = iterations.numpy()\n",
        "                    training_loss = self.train_loss.result() / (step + 1)\n",
        "\n",
        "                    logs = {}\n",
        "                    logs[\"training_loss\"] = training_loss.numpy()\n",
        "                    logs[\"learning_rate\"] = self.lr_scheduler(self.global_step).numpy()\n",
        "                    pbar(step=step, info=logs)\n",
        "\n",
        "                    if self.global_step % self.steps_per_epoch == 0:\n",
        "                        print(\"\\n------------- train result -----------------\")\n",
        "                        # call to evaluation loop\n",
        "                        self.evaluate()\n",
        "                        # save checkpoint\n",
        "                        ckpt_save_path = self.model.ckpt_manager.save()\n",
        "                        logger.info(f\"Saving checkpoint at {ckpt_save_path}\")\n",
        "                        break\n",
        "\n",
        "                # reset train loss after every epoch\n",
        "                self.train_loss.reset_states()\n",
        "            end_time = datetime.datetime.now()\n",
        "            logger.info(f\"Training took: {str(end_time - start_time)}\")"
      ],
      "metadata": {
        "id": "ri70eucB1cLD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(args):\n",
        "    logger.info(\" Starting training / evaluation\")\n",
        "\n",
        "    logger.info(\" Downloading Data Files\")\n",
        "    dataset_path = download_dataset(args.cache_dir)\n",
        "\n",
        "    logger.info(\" Loading Data Files\")\n",
        "    dataset = load_dataset('json', data_files=dataset_path)\n",
        "    # train test split\n",
        "    dataset = dataset['train'].train_test_split(0.1, shuffle=False)\n",
        "\n",
        "    logger.info(\" Initializing Tokenizer\")\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(args.tokenizer_name)\n",
        "\n",
        "    logger.info(\" Preparing Features\")\n",
        "    dataset = dataset.map(convert_examples_to_features, batched=True, fn_kwargs={\"tokenizer\":tokenizer, \"args\":args})\n",
        "\n",
        "    logger.info(\" Intializing training and validation dataset \")\n",
        "    train_dataset = dataset['train']\n",
        "    num_train_examples = len(dataset['train'])\n",
        "    # create tf train dataset\n",
        "    tf_train_dataset = get_train_tfdataset(train_dataset, num_train_examples, args)\n",
        "    validation_dataset = dataset['test']\n",
        "    num_validation_examples = len(dataset['test'])\n",
        "    # create tf validation dataset\n",
        "    tf_validation_dataset = get_validation_tfdataset(train_dataset, num_validation_examples, args)\n",
        "\n",
        "    logger.info(f' Intializing model | {args.model_type.upper()} ')\n",
        "    with strategy.scope():\n",
        "        # model must be created under `strategy.scope`\n",
        "        model = TFT5ForConditionalGeneration.from_pretrained(args.model_name_or_path, from_pt=True)\n",
        "\n",
        "    # custom training loop\n",
        "    trainer = Trainer(model, args, tf_train_dataset, tf_validation_dataset, num_train_examples, num_validation_examples)\n",
        "    trainer.train()\n",
        "\n",
        "    # save pretrained model and tokenizer\n",
        "    logger.info(f\" Saving model in {args.save_dir}\")\n",
        "    trainer.model.save_pretrained(args.save_dir)\n",
        "    tokenizer.save_pretrained(args.save_dir)"
      ],
      "metadata": {
        "id": "WtM037kN1cRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "    # define training arguments\n",
        "\n",
        "    # MODEL\n",
        "    model_type = 't5'\n",
        "    tokenizer_name = 'Salesforce/codet5-base'\n",
        "    model_name_or_path = 'Salesforce/codet5-base'\n",
        "\n",
        "    # DATA\n",
        "    train_batch_size = 8\n",
        "    validation_batch_size = 8\n",
        "    max_input_length = 48\n",
        "    max_target_length = 128\n",
        "    prefix = \"Generate Python: \"\n",
        "\n",
        "    # OPTIMIZER\n",
        "    learning_rate = 3e-4\n",
        "    weight_decay = 1e-4\n",
        "    warmup_ratio = 0.2\n",
        "    adam_epsilon = 1e-8\n",
        "# TRAINING\n",
        "    seed = 2022\n",
        "    epochs = 2\n",
        "\n",
        "    # DIRECTORIES\n",
        "    output_dir = \"runs/\"\n",
        "    logging_dir = f\"{output_dir}/logs/\"\n",
        "    checkpoint_dir = f\"checkpoint\"\n",
        "    save_dir = f\"{output_dir}/saved_model/\"\n",
        "    cache_dir = '../working/'\n",
        "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    Path(logging_dir).mkdir(parents=True, exist_ok=True)\n",
        "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "# initialize training arguments\n",
        "args = Args()\n",
        "# initialize logger\n",
        "logger = init_logger(log_file=os.path.join(args.logging_dir, f\"{args.model_type}-{time.strftime('%Y-%m-%d-%H-%M-%S', time.localtime())}.log\"))\n",
        "# fix all seeds\n",
        "fix_all_seeds(args.seed)\n",
        "if __name__ == \"__main__\":\n",
        "    # run training and evaluation\n",
        "    dataset = run(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550,
          "referenced_widgets": [
            "a7690623ec044d228b0bb3bb7e37f3ae",
            "f017d9ec51304d03b421f28ea5df64ec",
            "381fdf5782f34c16bd8c85697bdeb25c",
            "68d20171b12a4f1a911d5383a5270daa",
            "607ae5827be54712b802ae063a3af3f8",
            "64f8d0d8c01746e190e5c59e3bc4948d",
            "3f7fe4e13f614ff2b0cd948701be0c50",
            "b2f41f1b4c5e43efa1b96c202d37fa5d",
            "431ecff4cc124655a407c4c8ada5e6ca",
            "9fd76edd3ae5479d8b2f6212cbf5e85d",
            "ac22081c7ba5410cb9ad9d1a3d679550",
            "02bc4a30a7a5448ea72fec1b608cb446",
            "95055733eefb495e9d4005cd8948735b",
            "6315821ac76d460f8d6a88c32e155905",
            "2996b93b86cd458f8ece20b311e4dc67",
            "4dfa953210984cffb96396fcb6b878f6",
            "e4a19bc7a3cc452c83ec4b9d9c430467",
            "ab2eaba806af4d1b9bf7b31deb7663ef",
            "fc53fe16d7bd4636a39afa3de0b8d283",
            "60eeeacf93864fcd949289bb8c258014",
            "b4c7e4b7b26b4a8a90efa98e3f05ca73",
            "6365e973baec447f9f03d1cec933714f"
          ]
        },
        "id": "ptaS_cGs1cbY",
        "outputId": "32cd9d5b-f619-414f-a71c-68f88e86ed55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10/27/2023 18:23:08 - INFO - root -    Starting training / evaluation\n",
            "10/27/2023 18:23:08 - INFO - root -    Downloading Data Files\n",
            "10/27/2023 18:23:08 - INFO - root -    Loading Data Files\n",
            "10/27/2023 18:23:08 - INFO - root -    Initializing Tokenizer\n",
            "10/27/2023 18:23:09 - INFO - root -    Preparing Features\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/876 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7690623ec044d228b0bb3bb7e37f3ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/98 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02bc4a30a7a5448ea72fec1b608cb446"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10/27/2023 18:23:16 - INFO - root -    Intializing training and validation dataset \n",
            "10/27/2023 18:23:16 - INFO - root -    Intializing model | T5 \n",
            "10/27/2023 18:23:33 - INFO - root -   ***** Running training *****\n",
            "10/27/2023 18:23:33 - INFO - root -     Num examples = 876\n",
            "10/27/2023 18:23:33 - INFO - root -     Num Epochs = 2\n",
            "10/27/2023 18:23:33 - INFO - root -     Total train batch size (w. parallel & distributed) = 8\n",
            "10/27/2023 18:23:33 - INFO - root -     Steps per epoch = 109\n",
            "10/27/2023 18:23:33 - INFO - root -     Total optimization steps = 218\n",
            "10/27/2023 18:23:33 - INFO - root -   Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 27.6s/step  training_loss: 0.0046 - learning_rate: 0.00018793 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 11.1s/step  eval_loss: 0.0022 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10/27/2023 19:17:16 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-1\n",
            "10/27/2023 19:17:16 - INFO - root -   Epoch 2/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Training] 109/109 [==============================] 27.3s/step  training_loss: 0.0022 - learning_rate: 0.00000000 \n",
            "------------- train result -----------------\n",
            "[Evaluating] 13/13 [==============================] 10.8s/step  eval_loss: 0.0016 \n",
            "------------- validation result -----------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "10/27/2023 20:09:51 - INFO - root -   Saving checkpoint at runs/checkpoint/ckpt-2\n",
            "10/27/2023 20:10:38 - INFO - root -   Training took: 1:47:05.412358\n",
            "10/27/2023 20:10:38 - INFO - root -    Saving model in runs//saved_model/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_predict(args, text):\n",
        "    # load saved finetuned model\n",
        "    model = TFT5ForConditionalGeneration.from_pretrained(args.save_dir)\n",
        "    print(\"got model\")\n",
        "    # load saved tokenizer\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(args.save_dir)\n",
        "    print(\"got tokenizer\")\n",
        "     # encode texts by prepending the task for input sequence and appending the test sequence\n",
        "    query = args.prefix + text\n",
        "    encoded_text = tokenizer(query, return_tensors='tf', padding='max_length', truncation=True, max_length=args.max_input_length)\n",
        "    print(\"got encoded text\")\n",
        "    # inference\n",
        "    generated_code = model.generate(\n",
        "        encoded_text[\"input_ids\"], attention_mask=encoded_text[\"attention_mask\"],\n",
        "        max_length=args.max_target_length, top_p=0.95, top_k=50, repetition_penalty=2.0, num_return_sequences=1\n",
        "    )\n",
        "    print(\"got generate dcode\")\n",
        "    # decode generated tokens\n",
        "    decoded_code = tokenizer.decode(generated_code.numpy()[0], skip_special_tokens=True)\n",
        "    print(\"got decoded !!\")\n",
        "    return decoded_code\n",
        "\n",
        "def predict_from_dataset(args):\n",
        "    # load using hf datasets\n",
        "    dataset = load_dataset('json', data_files='../working/mbpp.jsonl')\n",
        "    # train test split\n",
        "    dataset = dataset['train'].train_test_split(0.1, shuffle=False)\n",
        "    test_dataset = dataset['test']\n",
        "\n",
        "    # randomly select an index from the validation dataset\n",
        "    index = random.randint(0, len(test_dataset))\n",
        "    text = test_dataset[index]['text']\n",
        "    code = test_dataset[index]['code']\n",
        "\n",
        "    # run-predict on text\n",
        "    decoded_code = run_predict(args, text)\n",
        "\n",
        "    print(\"#\" * 25); print(\"QUERY: \", text);\n",
        "    print()\n",
        "    print('#' * 25); print(\"ORIGINAL: \"); print(\"\\n\", code);\n",
        "    print()\n",
        "    print('#' * 25); print(\"GENERATED: \"); print(\"\\n\", decoded_code);\n",
        "\n",
        "def predict_from_text(args, text):\n",
        "    # run-predict on text\n",
        "    decoded_code = run_predict(args, text)\n",
        "    print(\"#\" * 25); print(\"QUERY: \", text);\n",
        "    print()\n",
        "    print('#' * 25); print(\"GENERATED: \"); print(\"\\n\", decoded_code);\n",
        "\n"
      ],
      "metadata": {
        "id": "ZhZJBmA61cfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example 1\n",
        "# predict_from_dataset(args)\n",
        "# # example 2\n",
        "# predict_from_dataset(args)\n",
        "# # example 3\n",
        "# predict_from_dataset(args)\n",
        "# predict_from_text(args, \"Write a function to add two random numbers\"); print()\n",
        "# example 2\n",
        "# predict_from_text(args, \"Write a function to find the frequency of items in a list\"); print()\n",
        "# # example 3\n",
        "# predict_from_text(args, \"Write a function to concatenate two dictionary\"); print()\n",
        "predict_from_text(args,\"write a function to read a number and print it \")\n",
        "predict_from_text(args,\"write a function to return the maximum of a list \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvvjCpUl210M",
        "outputId": "f86779b5-239b-4430-d580-55c39e5f0221"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "got model\n",
            "got tokenizer\n",
            "got encoded text\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "got generate dcode\n",
            "got decoded !!\n",
            "#########################\n",
            "QUERY:  write a function to read a number and print it \n",
            "\n",
            "#########################\n",
            "GENERATED: \n",
            "\n",
            " def read_number(n):\r\n",
            "  n = int((int)(math.sqrt('2')) + 1) \r\n",
            "got model\n",
            "got tokenizer\n",
            "got encoded text\n",
            "got generate dcode\n",
            "got decoded !!\n",
            "#########################\n",
            "QUERY:  write a function to return the maximum of a list \n",
            "\n",
            "#########################\n",
            "GENERATED: \n",
            "\n",
            " def max_list(nums):\n",
            "    return min([i for i in nums if isinstance((x, list))], 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HQtxH8uE2130"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kUZz0Em-217w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}